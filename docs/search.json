[
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "What are Generalized Additive Models (GAMs) and when should you use them?\n\n\n\n\n\nBuilding intuition on how GAMs work and when they are useful\n\n\n\n\n\nAug 22, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicole Sorensen",
    "section": "",
    "text": "Welcome to my website! I am a 3rd year undergraduate at Carnegie Mellon University, majoring in Statistics and Machine Learning. I’m interested in healthcare analytics."
  },
  {
    "objectID": "articles/gams/GAM_article.html",
    "href": "articles/gams/GAM_article.html",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "",
    "text": "GAMs are an advanced version of linear models that can handle nonlinear patterns in the data. Instead of assuming a straight-line relationship between the predictors and the outcome, GAMs use smooth curves to capture more complex relationships. The main ideas behind GAMs are:\n\nNonlinear predictor effects: The relationship between each predictor and the outcome is modeled using smooth curves, which can capture non-linear trends.\nAdditivity: The final prediction is made by adding up the effects of each predictor.\n\n\nThis is actually quite similar to linear regression! The main difference is that GAMs don’t require the relationship between each predictor and the outcome to be a straight line.\nEven though GAMs allow for non-linear effects, they are still considered linear models because the smooth functions are simply added together. The functions don’t interact or get multiplied in complicated ways, keeping the overall model linear.\nHere is how the two compare:\nIn linear models:\n\\[\nf(x) = \\beta _0 + \\beta_1x_1 + \\beta_2x_2 + \\dotsc + \\beta_nx_n\n\\]\nThe coefficients are just constants.\nIn GAMs:\n\\[\nf(x) = \\beta_0 + s_1(x_1) + s_2(x_2) + \\dots + s_p(x_p)\n\\]\nThe coefficients are replaced by functions denoted by \\(s_i\\). These functions are non-parametric meaning that each function \\(s_i\\) is learned from the data, without assuming a specific shape (like linear or quadratic). This makes GAMs more flexible compared to linear models, where each predictor is simply multiplied by a constant \\(\\beta_i\\).\nIn fact, linear models are just special cases of GAMs where each \\(s_i\\) is just a linear function instead of a smooth curve."
  },
  {
    "objectID": "articles/gams/GAM_article.html#what-are-gams",
    "href": "articles/gams/GAM_article.html#what-are-gams",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "",
    "text": "GAMs are an advanced version of linear models that can handle nonlinear patterns in the data. Instead of assuming a straight-line relationship between the predictors and the outcome, GAMs use smooth curves to capture more complex relationships. The main ideas behind GAMs are:\n\nNonlinear predictor effects: The relationship between each predictor and the outcome is modeled using smooth curves, which can capture non-linear trends.\nAdditivity: The final prediction is made by adding up the effects of each predictor.\n\n\nThis is actually quite similar to linear regression! The main difference is that GAMs don’t require the relationship between each predictor and the outcome to be a straight line.\nEven though GAMs allow for non-linear effects, they are still considered linear models because the smooth functions are simply added together. The functions don’t interact or get multiplied in complicated ways, keeping the overall model linear.\nHere is how the two compare:\nIn linear models:\n\\[\nf(x) = \\beta _0 + \\beta_1x_1 + \\beta_2x_2 + \\dotsc + \\beta_nx_n\n\\]\nThe coefficients are just constants.\nIn GAMs:\n\\[\nf(x) = \\beta_0 + s_1(x_1) + s_2(x_2) + \\dots + s_p(x_p)\n\\]\nThe coefficients are replaced by functions denoted by \\(s_i\\). These functions are non-parametric meaning that each function \\(s_i\\) is learned from the data, without assuming a specific shape (like linear or quadratic). This makes GAMs more flexible compared to linear models, where each predictor is simply multiplied by a constant \\(\\beta_i\\).\nIn fact, linear models are just special cases of GAMs where each \\(s_i\\) is just a linear function instead of a smooth curve."
  },
  {
    "objectID": "articles/gams/GAM_article.html#when-to-use-gams",
    "href": "articles/gams/GAM_article.html#when-to-use-gams",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "When to use GAMs?",
    "text": "When to use GAMs?\nIs a GAM the right choice for your problem? GAMs are useful when you see potential signs of nonlinearity in your data between a predictor variable and your response variable. This nonlinearity may show up in exploratory data analysis (EDA) with scatter plots that don’t look linear, or when you know from experience that the relationship isn’t linear.\nConsider a simple example: the relationship between age and the number of sit ups someone can perform. We might expect that people maintain their physical fitness into their 30s, but as they get older their physical ability starts to decline.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(datasets)\nlibrary(janitor)\nlibrary(gratia)\nlibrary(mgcv)\n\nperformance_df &lt;- read_csv(\"https://raw.githubusercontent.com/nicolesorense/GAM-article-resources/main/bodyPerformance.csv\")\n# performance_df &lt;- read_csv(\"/Users/nicolesorensen/Dropbox/bodyPerformance.csv\")\n\nperformance_df &lt;- performance_df |&gt; \n  clean_names()\n\nset.seed(64)\nsampled_data &lt;- performance_df |&gt; sample_n(1000)  # Sample for visibility\n\nsampled_data |&gt; \n  ggplot(aes(x = age, y = sit_ups_counts)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"gam\") +\n  labs(x = \"Age\", y = \"Number of sit-ups\", title = \"Number of sit-ups by age\")\n\n\n\n\n\n\n\n\n\nIn this case, a nonlinear pattern is obvious in the data so a linear model would fall short.\nTo see how the GAM model accounts for this nonlinear relationship, we can visualize the partial effect that age has on the outcome variable, predicted number of sit-ups.\n\n\nShow the code\ngam_hr &lt;- gam(sit_ups_counts ~ s(age), data = sampled_data)\npartial_effect_plot &lt;- draw(gam_hr, residuals = FALSE)\npartial_effect_plot +\n  labs(title = 'Partial effect of age on predicted number of sit-ups', \n       subtitle = 's(age)',\n       x = 'Age')\n\n\n\n\n\n\n\n\n\nThe graph above shows the smooth function of age to predict number of sit-ups. We see that people between 20 and 30 have a positive partial effect, so they increase the predicted number of sit-ups, while people over 30 show a decline, which aligns with our expectations that older people are likely to do less sit-ups.\nThese partial effect plots are a major interpretability advantage of GAMs over other nonlinear models. They show the individual effect of each predictor on the outcome without the complexity of interactions and high dimensional surfaces."
  },
  {
    "objectID": "articles/gams/GAM_article.html#when-not-to-use-gams",
    "href": "articles/gams/GAM_article.html#when-not-to-use-gams",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "When NOT to use GAMs?",
    "text": "When NOT to use GAMs?\nHow do I know when a GAM is not appropriate for my use case? Avoid GAMs when…\n\nYou need automatic interaction modeling: Since GAMs are additive, they might miss interactions between variables. While interaction terms can be added manually (as with linear regression), other models like random forests, gradient boosting machines, or support vector machines are better for capturing interactions automatically—especially if interpretability is not your primary concern.\nYou only care about prediction accuracy: In situations where prediction accuracy is your main goal, such as in Kaggle competitions, GAMs usually perform worse than more flexible models like XGBoost, LightGBM, or neural networks.\nAll relationships are expected to be linear: If your predictor-response relationships are expected to be linear, a linear regression model would suffice since it is simpler and more easily interpretable.\nThere is high concurvity between smooth functions: Concurvity is the GAM version of multicollinearity with linear regression. A GAM has high concurvity when one of the smooth functions is a sum of the others. This makes it difficult for the model to estimate the separate effects of predictors. You can check for concurvity using the concurvity() function in R, and if concurvity is high, this could be resolved by combining / removing predictors or switching to a different model."
  },
  {
    "objectID": "articles/gams/GAM_article.html#why-use-gams",
    "href": "articles/gams/GAM_article.html#why-use-gams",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "Why use GAMs?",
    "text": "Why use GAMs?\nLife is not linear!\nAlthough linear models are often sufficient to model many relationships, sometimes we do want to capture some complexity without oversimplifying it. GAMs are often advantageous over more complex models because their additive nature allows us to visualize the separate effects of each predictor on the outcome variable. This makes them much more interpretable than other “black-box” models like random forests and neural networks.\n\n(from Introduction to Statistical Learning)\nIn this above representation of the tradeoff between flexibility and interpretability from “An Introduction to Statistical Learning”, we see that GAMs are both relatively flexible and interpretable, making them good for both prediction and inference."
  },
  {
    "objectID": "articles/gams/GAM_article.html#how-are-the-functions-determined",
    "href": "articles/gams/GAM_article.html#how-are-the-functions-determined",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "How are the functions determined?",
    "text": "How are the functions determined?\nIn GAMs, the smooth functions are typically created by splines. Splines are just piecewise polynomial functions that meet specific conditions to ensure smoothness across different segments of data.\nWhy not use a single polynomial across all data?\nA single global polynomial can lead to poor fit, especially at the edges, and small changes in one part of the data can cause large changes elsewhere. This is called the Runge phenomenon, where high-degree polynomials oscillate and overfit in certain regions. Piecewise polynomials, or splines, resolve this by fitting local segments while ensuring smooth transitions across them.\nLet’s walk through the progression of how these splines evolve from simple piecewise polynomials to the smooth functions used in GAMs.\n\nStep 1: Piecewise Polynomials without Conditions\nWe can start by fitting piecewise cubic polynomials to different parts of the data. This means we divide the data into sections based on specific breakpoints and fit a cubic polynomial to each section independently.\n\n\n\n\n\n\n\n\n\nIn this example, we set two breakpoints at ages 30 and 60, fitting three separate cubic polynomials to different sections of the data. However, without additional conditions, there’s a major issue: the polynomials do not connect smoothly at the breakpoints, leading to discontinuities. For instance, the predicted number of sit-ups might suddenly drop from 27 to 18 at age 60, which is unrealistic.\n\n\nStep 2: Ensuring Continuity\nTo address this, we need to impose the condition that the function is continuous across the different segments. This means the pieces must meet at the breakpoints, so the function values are the same at the knots.\n\n\n\n\n\n\n\n\n\nAfter adding this continuity constraint, the splines are connected, and the function appears mostly smooth. However, in many cases, the function may still have abrupt changes at the breakpoints, or knots, even though it is continuous, looking something like this:\n\n\n\nStep 3: Ensuring Smoothness\nTo create a smoother curve, we need to go a step further. In addition to making the function continuous, we also ensure the first and second derivatives (slope and curvature) are continuous at the breakpoints. This prevents sudden changes in the shape of the curve. GAMs do this automatically by using cubic splines.\n\n\nShow the code\n# Fit a GAM model using cubic splines on the sampled data\nfit_spline_sampled &lt;- gam(sit_ups_counts ~ s(age), data = sampled_data)\n\n# Generate predictions from the model for the sampled data\nsampled_data &lt;- sampled_data |&gt; \n  mutate(predicted_sit_ups = predict(fit_spline_sampled, newdata = sampled_data))\n\n# Plot the sampled data and the fitted spline\np_spline_sampled &lt;- sampled_data |&gt; \n  ggplot(aes(x = age, y = sit_ups_counts)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = predicted_sit_ups), color = \"blue\", size = 1) +\n  labs(x = \"Age\", y = \"Number of sit-ups\", title = \"Cubic Spline Fit\")\n\np_spline_sampled\n\n\n\n\n\n\n\n\n\nWith these conditions, the function is now both continuous and smooth. The process of fitting these smooth splines is automated in tools like the mgcv package in R, which makes it easy to apply.\nHow to choose the number of knots?\nIt’s important to choose the right number of knots when fitting splines. Too few knots can make the model too rigid, while too many can lead to overfitting. Typically, the best number of knots is selected using cross-validation, which helps balance model complexity and performance."
  },
  {
    "objectID": "articles/gams/GAM_article.html#how-to-interpret-gams",
    "href": "articles/gams/GAM_article.html#how-to-interpret-gams",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "How to interpret GAMs?",
    "text": "How to interpret GAMs?\nAlthough GAMs are more interpretable than more complex nonparametric models, they can still sometimes be hard to understand.\nContinuing with previous example, let’s add body fat percent to the model so that now there are now multiple predictor variables. The model equation is now of the form:\n\\[f(x) = \\beta_0 + s_1(\\mathrm{age}) + s_2(\\mathrm{body\\_fat\\_percent})\\]\n\n\nShow the code\nlibrary(patchwork)\ngam_multiple &lt;- gam(sit_ups_counts ~ s(age) + s(body_fat_percent), data = sampled_data)\npartial_effect_plot_multiple &lt;- draw(gam_multiple, residuals = FALSE)\nwrap_plots(partial_effect_plot_multiple) +\n  plot_annotation(title = \"Partial effects of predictors on number of push-ups\")\n\n\n\n\n\n\n\n\n\nThese partial effects plots show how much the variable on the x-axis increases or decreases the predicted response, assuming all the other variables stay constant.\nFor example, in the body fat plot, people with low body fat have a positive effect, meaning they can do more sit-ups. In contrast, people with higher body fat have a negative effect, meaning they tend to do fewer sit-ups.\n\nCategorical variables\nGAMs handle categorical variables just like linear regression—using dummy variables. This means one category is chosen as the baseline, and all other categories are compared to that baseline.\nFor example, here’s how the model handles gender:\n\n\nShow the code\nsampled_data &lt;- sampled_data |&gt; \n  mutate(gender = as.factor(gender))\n\ngam_cat &lt;- gam(sit_ups_counts ~ gender, data = sampled_data, method = \"REML\")\n\npartial_effect_plot_cat &lt;- draw(gam_cat, residuals = FALSE, parametric = TRUE)\npartial_effect_plot_cat +\n  labs(title = \"Partial effects of gender on number of sit-ups\",\n       x = \"Gender\")\n\n\n\n\n\n\n\n\n\nIn this case, female is the baseline, and the graph shows that males tend to do more sit-ups than females, with a positive partial effect. We can interpret this as:\n“Males can do about 14 more sit-ups than females, assuming everything else is constant.”\nThe confidence interval shows we are 95% confident that the true difference is between 12 and 15 extra sit-ups.\n\n\nWhat about interactions?\nGAMs don’t automatically include interaction effects, but we can manually add them if we think the effect of one variable might depend on another.\nFor example, younger people with higher body fat might still perform well because they have more muscle and faster metabolisms. For older people, high body fat may have a stronger negative effect. Thus, we expect that the effect of body fat on number of sit-ups increases with age.\nTo model this interaction, we can include both age and body fat percentage together.\n\n\nShow the code\ngam_interact &lt;- gam(sit_ups_counts ~ te(age, body_fat_percent), data = sampled_data)\npartial_effect_plot_interact &lt;- draw(gam_interact, residuals = FALSE)\npartial_effect_plot_interact +\n  labs(caption = \"T\")\n\n\n\n\n\n\n\n\n\nIn this plot, the contour lines show the interaction between age and body fat. If the lines were straight, it would mean that age and body fat are affecting sit-ups independently. However, the curved lines suggest that the effect of body fat on sit-ups depends on a person’s age, and vice versa.\nFor example, body fat doesn’t have much effect for younger people (flat lines), but it has a stronger negative effect for older people (steeper lines).\nIf the variables are on different scales, you can use te() (tensor product) to model their interaction instead of s()."
  },
  {
    "objectID": "articles/gams/GAM_article.html#model-assumptions",
    "href": "articles/gams/GAM_article.html#model-assumptions",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "Model assumptions",
    "text": "Model assumptions\nGAMs make several assumptions, some of which are similar to linear regression, while others are different.\n\nSmooth functions instead of linear relationships:\nUnlike linear regression where the predictors have linear relationships with the outcomes, for GAMs, the functions don’t have to be linear and can be anything that can be drawn as a squiggly line on a graph.\n\n\n\nModel\nRelationship type\n\n\n\n\nLinear regression\nPredictors must be linear\n\n\nGAMs\nPredictors can be nonlinear\n\n\n\nHow to check: Visualize the smooth functions using partial effects plots to see if the shape of the smooth functions make sense given your data.\nAdditivity of effects:\nJust as with linear regression, the effects of each of the variables on the outcome are added together to get the response.\n\n\n\nModel\nHow effects are combined\n\n\n\n\nLinear regression\nEffects are added together\n\n\nGAMs\nEffects are added together\n\n\n\nHow to check: There is no direct diagnostic for this, but if you suspect interactions, try adding interaction terms and see if they significantly improve the model fit. If they do, the additivity assumption is insufficient.\nIndependence of errors:\nSame as with linear regression, GAMs assume that the errors are independent of each other. This helps ensure that no important patterns or variables are missing from the model.\n\n\n\nModel\nAssumption\n\n\n\n\nLinear regression\nErrors should be independent\n\n\nGAMs\nErrors should be independent\n\n\n\nHow to check: plot the residuals against fitted values, or if you are using time series data, the Durbin-Watson test will check for autocorrelation of the residuals.\nDistribution of errors:\nThe required distribution of errors in GAMs depends on the type of outcome variable:\n\nFor a continuous response, the errors should be normally distributed.\nFor a binary response, the errors should follow a binomial distribution.\nFor count data, the errors should follow a Poisson distribution.\n\n\n\n\n\n\n\n\nModel\nRelationship type\n\n\n\n\nLinear regression\nNormally distributed errors (if running hypothesis tests or calculating confidence intervals)\n\n\nGAMs\nDistribution depends on distribution of response\n\n\n\nHow to check: If the outcome is continuous (like the number of sit-ups), we check if the errors are normally distributed by looking at QQ plots and histograms of residuals."
  },
  {
    "objectID": "articles/gams/GAM_article.html#conclusion",
    "href": "articles/gams/GAM_article.html#conclusion",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "Conclusion",
    "text": "Conclusion\nThe purpose of this article was to help build more intuition about how GAMs work and to help people determine if GAMs are appropriate for their use case.\nWe saw how GAMs are a balanced model in the sense that they are flexible enough to capture more complicated trends while still being simple enough where they can be easily interpreted. While GAMs are powerful, they do have their limitations like needing to handle interactions manually and needing to have a large enough data set to accurately capture the nonlinear patterns."
  },
  {
    "objectID": "articles/gams/GAM_article.html#references",
    "href": "articles/gams/GAM_article.html#references",
    "title": "What are Generalized Additive Models (GAMs) and when should you use them?",
    "section": "References",
    "text": "References\nBasheer, K. C. S. (2024, August 12). Understanding generalized additive models (GAMS): A comprehensive guide. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2023/09/understanding-generalized-additive-models-gams-a-comprehensive-guide/\nBottom of the Heap. (2020, July 30). Introduction to Generalized Additive Models with R and mgcv. YouTube. https://www.youtube.com/watch?v=sgw4cu8hrZM&t=957s\nDepartment of Statistics & Data Science Carnegie Mellon University. (2024, July 2). Supervised learning: Nonparametric regression. SURE 2024. https://36-sure.github.io/lectures/18-nonparametric.html#/title-slide\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An introduction to statistical learning: With applications in R. Springer.\nLarsen, K. (2015, July 30). Gam: The predictive modeling silver bullet. MultiThreaded. https://multithreaded.stitchfix.com/blog/2015/07/30/gam/\nQuébec Centre for Biodiversity Science. (2023, April). Workshop 8: Generalized additive models. QCBS R Workshop Series.\nShafi, A. (2021, May 18). What is a generalised additive model?. Medium. https://towardsdatascience.com/generalised-additive-models-6dfbedf1350a\nSimpson, G. (2022, January 7). Statistical Methods Series: Generalized Additive Models (GAMs). YouTube. https://www.youtube.com/watch?v=Ukfvd8akfco"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Below, you can find my projects…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSocioeconomic roots of racial disparities in hospitalizations\n\n\n\n\n\nModeling preventable hospital stays using Generalized Additive Models and Random Forests\n\n\n\n\n\nJul 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 dynamics of Pennsylvania\n\n\n\n\n\nHierarchical clustering and EDA to examine trends in COVID-19 cases and deaths for counties in Pennsylvania during 2020\n\n\n\n\n\nJun 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAlpacalypse\n\n\n\n\n\nSurvival video game coded in python using object-oriented programming and algorithms including A* pathfinding and cellular automata\n\n\n\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDepression and health\n\n\n\n\n\nData visualization to examine what factors influence depression and health over time\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]